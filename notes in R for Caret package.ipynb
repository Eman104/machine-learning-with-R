{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6333e03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "Loading required package: ggplot2\n",
      "Registered S3 methods overwritten by 'ggplot2':\n",
      "  method         from \n",
      "  [.quosures     rlang\n",
      "  c.quosures     rlang\n",
      "  print.quosures rlang\n"
     ]
    }
   ],
   "source": [
    "# Load the caret package\n",
    "library(caret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abd082a",
   "metadata": {},
   "source": [
    "# caret Package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f67be4",
   "metadata": {},
   "source": [
    "### A Short Introduction to the caret Package\n",
    "\n",
    "The caret package (short for Classification And REgression Training) contains functions to streamline the model training process for complex regression and classification problems. The package utilizes a number of R packages but tries not to load them all at package start-up (by removing formal package dependencies, the package startup time can be greatly decreased). The package “suggests” field includes 32 packages. caret loads packages as needed and assumes that they are installed. If a modeling package is missing, there is a prompt to install it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc4bc61",
   "metadata": {},
   "source": [
    "### caret has several functions that attempt to streamline the model building and evaluation process, as well as feature selection and other techniques.\n",
    "\n",
    "One of the primary tools in the package is the train function which can be used to\n",
    "\n",
    "evaluate, using resampling, the effect of model tuning parameters on performance\n",
    "choose the ``optimal’’ model across these parameters\n",
    "estimate model performance from a training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e4923d",
   "metadata": {},
   "source": [
    "## split the data\n",
    "a training set and a test set. To do this, the createDataPartition function is used:\n",
    "\n",
    "By default, createDataPartition does a stratified random split of the data. To partition the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inTrain <- createDataPartition(\n",
    "  y = Sonar$Class,\n",
    "  ## the outcome data are needed\n",
    "  p = .75,\n",
    "  ## The percentage of data in the\n",
    "  ## training set\n",
    "  list = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4354dcd4",
   "metadata": {},
   "source": [
    "# another shape for split data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe17e4a4",
   "metadata": {},
   "source": [
    "### createDataPartition()\n",
    "\n",
    "takes as input the Y variable in the source dataset and the percentage data that should go into training as the p argument. It returns the rownumbers that should form the training dataset.\n",
    "\n",
    "Plus, you need to set list=F, to prevent returning the result as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c8dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and test datasets\n",
    "set.seed(100)\n",
    "\n",
    "# Step 1: Get row numbers for the training data\n",
    "trainRowNumbers <- createDataPartition(orange$Purchase, p=0.8, list=FALSE)\n",
    "\n",
    "# Step 2: Create the training  dataset\n",
    "trainData <- orange[trainRowNumbers,]\n",
    "\n",
    "# Step 3: Create the test dataset\n",
    "testData <- orange[-trainRowNumbers,]\n",
    "\n",
    "# Store X and Y for later use.\n",
    "x = trainData[, 2:18]\n",
    "y = trainData$Purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f9890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cb796b5",
   "metadata": {},
   "source": [
    "## Descriptive statistics\n",
    "\n",
    "Before moving to missing value imputation and feature preprocessing, let’s observe the descriptive statistics of each column in the training dataset.\n",
    "\n",
    "The skimr package provides a nice solution to show key descriptive stats for each column.\n",
    "\n",
    "The skimr::skim_to_wide() produces a nice dataframe containing the descriptive stats of each of the columns. The dataframe output includes a nice histogram drawn without any plotting help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95087237",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(skimr)\n",
    "skimmed <- skim_to_wide(trainData)\n",
    "skimmed[, c(1:5, 9:11, 13, 15:16)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d421f3aa",
   "metadata": {},
   "source": [
    "# How to impute missing values using preProcess()?\n",
    "\n",
    "We’ve seen that the dataset has few missing values across all columns, we may to do well to impute it. Impute, means to fill it up with some meaningful values.\n",
    "\n",
    "\n",
    "If the feature is a continuous variable, it is a common practice to replace the missing values with the mean of the column. And if it’s a categorical variable, replace the missings with the most frequently occurring value, aka, the mode.\n",
    "\n",
    "But this is quite a basic and a rather rudimentary approach.\n",
    "\n",
    "Instead what can be done is, you can actually predict the missing values by considering the rest of the available variables as predictors. A popular algorithm to do imputation is the k-Nearest Neighbors. This can be quickly and easily be done using caret.\n",
    "\n",
    "### To predict the missing values with k-Nearest Neighbors using preProcess():\n",
    "\n",
    "1-You need to set the method=knnImpute for k-Nearest Neighbors and apply it on the training data. This creates a preprocess model.\n",
    "\n",
    "2-Then use predict() on the created preprocess model by setting the newdata argument on the same training data.\n",
    "\n",
    "Caret also provides bagImpute as an alternative imputation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bcc1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#               first step\n",
    "# Create the knn imputation model on the training data\n",
    "preProcess_missingdata_model <- preProcess(trainData, method='knnImpute')\n",
    "preProcess_missingdata_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a42d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         seond step\n",
    "# Use the imputation model to predict the values of missing data points\n",
    "library(RANN)  # required for knnInpute\n",
    "trainData <- predict(preProcess_missingdata_model, newdata = trainData)\n",
    "anyNA(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ded92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c71518d",
   "metadata": {},
   "source": [
    "# How to create One-Hot Encoding (dummy variables)?\n",
    "\n",
    "Let me first explain what is one-hot encoding and why it is required.\n",
    "\n",
    "Suppose if you have a categorical column as one of the features, it needs to be converted to numeric in order for it to be used by the machine learning algorithms.\n",
    "\n",
    "In caret, one-hot-encodings can be created using dummyVars(). Just pass in all the features to dummyVars() as the training data and all the factor columns will automatically be converted to one-hot-encodings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a78fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding\n",
    "# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.\n",
    "dummies_model <- dummyVars(Purchase ~ ., data=trainData)\n",
    "\n",
    "# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.\n",
    "trainData_mat <- predict(dummies_model, newdata = trainData)\n",
    "\n",
    "# # Convert to dataframe\n",
    "trainData <- data.frame(trainData_mat)\n",
    "\n",
    "# # See the structure of the new dataset\n",
    "str(trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61149d1",
   "metadata": {},
   "source": [
    "In above case, we had one categorical variable, Store7 with 2 categories. It was one-hot-encoded to produce two new columns – Store7.No and Store7.Yes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf44d93c",
   "metadata": {},
   "source": [
    "# How to preprocess to transform the data?\n",
    "With the missing values handled and the factors one-hot-encoded, our training dataset is now ready to undergo variable transformations if required.\n",
    "\n",
    "So what type of preprocessing are available in caret?\n",
    "\n",
    "### range:\n",
    "Normalize values so it ranges between 0 and 1\n",
    "\n",
    "### center: \n",
    "Subtract Mean\n",
    "\n",
    "### scale:\n",
    "Divide by standard deviation\n",
    "\n",
    "### BoxCox: \n",
    "Remove skewness leading to normality. Values must be > 0\n",
    "\n",
    "### YeoJohnson:\n",
    "Like BoxCox, but works for negative values.\n",
    "\n",
    "### expoTrans:\n",
    "Exponential transformation, works for negative values.\n",
    "\n",
    "### pca:\n",
    "Replace with principal components\n",
    "\n",
    "### ica: \n",
    "Replace with independent components\n",
    "\n",
    "### spatialSign:\n",
    "Project the data to a unit circle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1321bed7",
   "metadata": {},
   "source": [
    "For our problem, let’s convert all the numeric variables to range between 0 and 1, by setting method=range in preProcess()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b769e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preProcess_range_model <- preProcess(trainData, method='range')\n",
    "trainData <- predict(preProcess_range_model, newdata = trainData)\n",
    "\n",
    "# Append the Y variable\n",
    "trainData$Purchase <- y\n",
    "\n",
    "apply(trainData[, 1:10], 2, FUN=function(x){c('min'=min(x), 'max'=max(x))})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c4742e",
   "metadata": {},
   "source": [
    "## How to visualize the importance of variables using featurePlot()\n",
    "\n",
    "Now that the preprocessing is complete, let’s visually examine how the predictors influence the Y (Purchase).\n",
    "\n",
    "In this problem, the X variables are numeric whereas the Y is categorical.\n",
    "\n",
    "So how to gauge if a given X is an important predictor of Y?\n",
    "\n",
    "A simple common sense approach is, if you group the X variable by the categories of Y, a significant mean shift amongst the X’s groups is a strong indicator (if not the only indicator) that X will have a significant role to help predict Y.\n",
    "\n",
    "It is possible to watch this shift visually using box plots and density plots.\n",
    "\n",
    "In fact, caret’s featurePlot() function makes it so convenient.\n",
    "\n",
    "Simply set the X and Y parameters and set plot='box'. You can additionally adjust the label font size (using strip) and the scales to be free as I have done in the below plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abfcd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  strip and scale are additional \n",
    "featurePlot(x = trainData[, 1:18], \n",
    "            y = trainData$Purchase, \n",
    "            plot = \"box\",\n",
    "            strip=strip.custom(par.strip.text=list(cex=.7)),\n",
    "            scales = list(x = list(relation=\"free\"), \n",
    "                          y = list(relation=\"free\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37be5dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurePlot(x = trainData[, 1:18], \n",
    "            y = trainData$Purchase, \n",
    "            plot = \"density\",\n",
    "            strip=strip.custom(par.strip.text=list(cex=.7)),\n",
    "            scales = list(x = list(relation=\"free\"), \n",
    "                          y = list(relation=\"free\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e90cf52",
   "metadata": {},
   "source": [
    "# correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c527fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(corrplot)\n",
    "M <- cor(numeric_mydata)\n",
    "M\n",
    "corrplot(M, ,method='square',type='upper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71175161",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding how many correlations are bigger than 0.70\n",
    "k = 0\n",
    "for(i in 1:25){\n",
    "for(r in 1:25){\n",
    "  if(M[i,r]> 0.70 & i != r){\n",
    "    print(rownames(M)[i])\n",
    "    print(colnames(M)[r])\n",
    "    print(M[i,r])\n",
    "    k= k + 1\n",
    "  }\n",
    "}  }\n",
    "print(k/2)\n",
    "\n",
    "options( warn = -1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d9739",
   "metadata": {},
   "source": [
    "# How to do feature selection using recursive feature elimination (rfe)?\n",
    "\n",
    "Most machine learning algorithms are able to determine what features are important to predict the Y. But in some scenarios, you might be need to be careful to include only variables that may be significantly important and makes strong business sense.\n",
    "\n",
    "This is quite common in banking, economics and financial institutions.\n",
    "\n",
    "Or you might just be doing an exploratory analysis to determine important predictors and report it as a metric in your analytics dashboard.\n",
    "\n",
    "Or if you are using a traditional algorithm like like linear or logistic regression, determining what variable to feed to the model is in the hands of the practitioner.\n",
    "\n",
    "Given such requirements, you might need a rigorous way to determine the important variables first before feeding them to the ML algorithm.\n",
    "\n",
    "A good choice of selecting the important features is the recursive feature elimination (RFE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f83f5",
   "metadata": {},
   "source": [
    "# RFE works in 3 broad steps:\n",
    "\n",
    "Step 1: Build a ML model on a training dataset and estimate the feature importances on the test dataset.\n",
    "\n",
    "Step 2: Keeping priority to the most important variables, iterate through by building models of given subset sizes, that is, subgroups of most important predictors determined from step 1. Ranking of the predictors is recalculated in each iteration.\n",
    "\n",
    "Step 3: The model performances are compared across different subset sizes to arrive at the optimal number and list of final predictors.\n",
    "\n",
    "It can be implemented using the rfe() function and you have the flexibility to control what algorithm rfe uses and how it cross validates by defining the rfeControl().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aef6184",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ctrl <- rfeControl(functions = rfFuncs,\n",
    "                   method = \"repeatedcv\",\n",
    "                   repeats = 5,\n",
    "                   verbose = FALSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba7e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets <- c(1:5, 10, 15, 18)\n",
    "\n",
    "lmProfile <- rfe(x=trainData[, 1:18], y=trainData$Purchase,\n",
    "                 sizes = subsets,\n",
    "                 rfeControl = ctrl)\n",
    "\n",
    "lmProfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8280987a",
   "metadata": {},
   "source": [
    "n the above code, we call the rfe() which implements the recursive feature elimination.\n",
    "\n",
    "Apart from the x and y datasets, RFE also takes two important parameters.\n",
    "\n",
    "sizes\n",
    "rfeControl\n",
    "The sizes determines what all model sizes (the number of most important features) the rfe should consider. In above case, it iterates models of size 1 to 5, 10, 15 and 18.\n",
    "\n",
    "The rfeControl parameter on the other hand receives the output of the rfeControl() as values. If you look at the call to rfeControl() we set what type of algorithm and what cross validation method should be used.\n",
    "\n",
    "In above case, the cross validation method is repeatedcv which implements k-Fold cross validation repeated 5 times, which is rigorous enough for our case.\n",
    "\n",
    "Once rfe() is run, the output shows the accuracy and kappa (and their standard deviation) for the different model sizes we provided. The final selected model subset size is marked with a * in the rightmost Selected column.\n",
    "\n",
    "From the above output, a model size of 3 with LoyalCH, PriceDiff and StoreID seems to achieve the optimal accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c854e34",
   "metadata": {},
   "source": [
    "# hi\n",
    "\n",
    "That means, out of 18 other features, a model with just 3 features outperformed many other larger model. Interesting isn’t it! Can you explain why?\n",
    "\n",
    "However, it is not a mandate that only including these 3 variables will always give high accuracy over larger sized models.\n",
    "\n",
    "Thats because, the rfe() we just implemented is particular to random forest based rfFuncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd66baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee818904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d9fe0e7",
   "metadata": {},
   "source": [
    "# n the next step, we will build the actual randomForest model on trainData\n",
    "\n",
    "## How to train() the model and interpret the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b637e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'ada,  AdaBag,  AdaBoost.M1,  adaboost,  amdai,  ANFIS,  avNNet,  awnb,  awtan,  bag,  bagEarth,  bagEarthGCV,  bagFDA,  bagFDAGCV,  bam,  bartMachine,  bayesglm,  binda,  blackboost,  blasso,  blassoAveraged,  bridge,  brnn,  BstLm,  bstSm,  bstTree,  C5.0,  C5.0Cost,  C5.0Rules,  C5.0Tree,  cforest,  chaid,  CSimca,  ctree,  ctree2,  cubist,  dda,  deepboost,  DENFIS,  dnn,  dwdLinear,  dwdPoly,  dwdRadial,  earth,  elm,  enet,  evtree,  extraTrees,  fda,  FH.GBML,  FIR.DM,  foba,  FRBCS.CHI,  FRBCS.W,  FS.HGD,  gam,  gamboost,  gamLoess,  gamSpline,  gaussprLinear,  gaussprPoly,  gaussprRadial,  gbm_h2o,  gbm,  gcvEarth,  GFS.FR.MOGUL,  GFS.LT.RS,  GFS.THRIFT,  glm.nb,  glm,  glmboost,  glmnet_h2o,  glmnet,  glmStepAIC,  gpls,  hda,  hdda,  hdrda,  HYFIS,  icr,  J48,  JRip,  kernelpls,  kknn,  knn,  krlsPoly,  krlsRadial,  lars,  lars2,  lasso,  lda,  lda2,  leapBackward,  leapForward,  leapSeq,  Linda,  lm,  lmStepAIC,  LMT,  loclda,  logicBag,  LogitBoost,  logreg,  lssvmLinear,  lssvmPoly,  lssvmRadial,  lvq,  M5,  M5Rules,  manb,  mda,  Mlda,  mlp,  mlpKerasDecay,  mlpKerasDecayCost,  mlpKerasDropout,  mlpKerasDropoutCost,  mlpML,  mlpSGD,  mlpWeightDecay,  mlpWeightDecayML,  monmlp,  msaenet,  multinom,  mxnet,  mxnetAdam,  naive_bayes,  nb,  nbDiscrete,  nbSearch,  neuralnet,  nnet,  nnls,  nodeHarvest,  null,  OneR,  ordinalNet,  ordinalRF,  ORFlog,  ORFpls,  ORFridge,  ORFsvm,  ownn,  pam,  parRF,  PART,  partDSA,  pcaNNet,  pcr,  pda,  pda2,  penalized,  PenalizedLDA,  plr,  pls,  plsRglm,  polr,  ppr,  PRIM,  protoclass,  qda,  QdaCov,  qrf,  qrnn,  randomGLM,  ranger,  rbf,  rbfDDA,  Rborist,  rda,  regLogistic,  relaxo,  rf,  rFerns,  RFlda,  rfRules,  ridge,  rlda,  rlm,  rmda,  rocc,  rotationForest,  rotationForestCp,  rpart,  rpart1SE,  rpart2,  rpartCost,  rpartScore,  rqlasso,  rqnc,  RRF,  RRFglobal,  rrlda,  RSimca,  rvmLinear,  rvmPoly,  rvmRadial,  SBC,  sda,  sdwd,  simpls,  SLAVE,  slda,  smda,  snn,  sparseLDA,  spikeslab,  spls,  stepLDA,  stepQDA,  superpc,  svmBoundrangeString,  svmExpoString,  svmLinear,  svmLinear2,  svmLinear3,  svmLinearWeights,  svmLinearWeights2,  svmPoly,  svmRadial,  svmRadialCost,  svmRadialSigma,  svmRadialWeights,  svmSpectrumString,  tan,  tanSearch,  treebag,  vbmpRadial,  vglmAdjCat,  vglmContRatio,  vglmCumulative,  widekernelpls,  WM,  wsrf,  xgbDART,  xgbLinear,  xgbTree,  xyf'</span>"
      ],
      "text/latex": [
       "'ada,  AdaBag,  AdaBoost.M1,  adaboost,  amdai,  ANFIS,  avNNet,  awnb,  awtan,  bag,  bagEarth,  bagEarthGCV,  bagFDA,  bagFDAGCV,  bam,  bartMachine,  bayesglm,  binda,  blackboost,  blasso,  blassoAveraged,  bridge,  brnn,  BstLm,  bstSm,  bstTree,  C5.0,  C5.0Cost,  C5.0Rules,  C5.0Tree,  cforest,  chaid,  CSimca,  ctree,  ctree2,  cubist,  dda,  deepboost,  DENFIS,  dnn,  dwdLinear,  dwdPoly,  dwdRadial,  earth,  elm,  enet,  evtree,  extraTrees,  fda,  FH.GBML,  FIR.DM,  foba,  FRBCS.CHI,  FRBCS.W,  FS.HGD,  gam,  gamboost,  gamLoess,  gamSpline,  gaussprLinear,  gaussprPoly,  gaussprRadial,  gbm\\_h2o,  gbm,  gcvEarth,  GFS.FR.MOGUL,  GFS.LT.RS,  GFS.THRIFT,  glm.nb,  glm,  glmboost,  glmnet\\_h2o,  glmnet,  glmStepAIC,  gpls,  hda,  hdda,  hdrda,  HYFIS,  icr,  J48,  JRip,  kernelpls,  kknn,  knn,  krlsPoly,  krlsRadial,  lars,  lars2,  lasso,  lda,  lda2,  leapBackward,  leapForward,  leapSeq,  Linda,  lm,  lmStepAIC,  LMT,  loclda,  logicBag,  LogitBoost,  logreg,  lssvmLinear,  lssvmPoly,  lssvmRadial,  lvq,  M5,  M5Rules,  manb,  mda,  Mlda,  mlp,  mlpKerasDecay,  mlpKerasDecayCost,  mlpKerasDropout,  mlpKerasDropoutCost,  mlpML,  mlpSGD,  mlpWeightDecay,  mlpWeightDecayML,  monmlp,  msaenet,  multinom,  mxnet,  mxnetAdam,  naive\\_bayes,  nb,  nbDiscrete,  nbSearch,  neuralnet,  nnet,  nnls,  nodeHarvest,  null,  OneR,  ordinalNet,  ordinalRF,  ORFlog,  ORFpls,  ORFridge,  ORFsvm,  ownn,  pam,  parRF,  PART,  partDSA,  pcaNNet,  pcr,  pda,  pda2,  penalized,  PenalizedLDA,  plr,  pls,  plsRglm,  polr,  ppr,  PRIM,  protoclass,  qda,  QdaCov,  qrf,  qrnn,  randomGLM,  ranger,  rbf,  rbfDDA,  Rborist,  rda,  regLogistic,  relaxo,  rf,  rFerns,  RFlda,  rfRules,  ridge,  rlda,  rlm,  rmda,  rocc,  rotationForest,  rotationForestCp,  rpart,  rpart1SE,  rpart2,  rpartCost,  rpartScore,  rqlasso,  rqnc,  RRF,  RRFglobal,  rrlda,  RSimca,  rvmLinear,  rvmPoly,  rvmRadial,  SBC,  sda,  sdwd,  simpls,  SLAVE,  slda,  smda,  snn,  sparseLDA,  spikeslab,  spls,  stepLDA,  stepQDA,  superpc,  svmBoundrangeString,  svmExpoString,  svmLinear,  svmLinear2,  svmLinear3,  svmLinearWeights,  svmLinearWeights2,  svmPoly,  svmRadial,  svmRadialCost,  svmRadialSigma,  svmRadialWeights,  svmSpectrumString,  tan,  tanSearch,  treebag,  vbmpRadial,  vglmAdjCat,  vglmContRatio,  vglmCumulative,  widekernelpls,  WM,  wsrf,  xgbDART,  xgbLinear,  xgbTree,  xyf'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'ada,  AdaBag,  AdaBoost.M1,  adaboost,  amdai,  ANFIS,  avNNet,  awnb,  awtan,  bag,  bagEarth,  bagEarthGCV,  bagFDA,  bagFDAGCV,  bam,  bartMachine,  bayesglm,  binda,  blackboost,  blasso,  blassoAveraged,  bridge,  brnn,  BstLm,  bstSm,  bstTree,  C5.0,  C5.0Cost,  C5.0Rules,  C5.0Tree,  cforest,  chaid,  CSimca,  ctree,  ctree2,  cubist,  dda,  deepboost,  DENFIS,  dnn,  dwdLinear,  dwdPoly,  dwdRadial,  earth,  elm,  enet,  evtree,  extraTrees,  fda,  FH.GBML,  FIR.DM,  foba,  FRBCS.CHI,  FRBCS.W,  FS.HGD,  gam,  gamboost,  gamLoess,  gamSpline,  gaussprLinear,  gaussprPoly,  gaussprRadial,  gbm_h2o,  gbm,  gcvEarth,  GFS.FR.MOGUL,  GFS.LT.RS,  GFS.THRIFT,  glm.nb,  glm,  glmboost,  glmnet_h2o,  glmnet,  glmStepAIC,  gpls,  hda,  hdda,  hdrda,  HYFIS,  icr,  J48,  JRip,  kernelpls,  kknn,  knn,  krlsPoly,  krlsRadial,  lars,  lars2,  lasso,  lda,  lda2,  leapBackward,  leapForward,  leapSeq,  Linda,  lm,  lmStepAIC,  LMT,  loclda,  logicBag,  LogitBoost,  logreg,  lssvmLinear,  lssvmPoly,  lssvmRadial,  lvq,  M5,  M5Rules,  manb,  mda,  Mlda,  mlp,  mlpKerasDecay,  mlpKerasDecayCost,  mlpKerasDropout,  mlpKerasDropoutCost,  mlpML,  mlpSGD,  mlpWeightDecay,  mlpWeightDecayML,  monmlp,  msaenet,  multinom,  mxnet,  mxnetAdam,  naive_bayes,  nb,  nbDiscrete,  nbSearch,  neuralnet,  nnet,  nnls,  nodeHarvest,  null,  OneR,  ordinalNet,  ordinalRF,  ORFlog,  ORFpls,  ORFridge,  ORFsvm,  ownn,  pam,  parRF,  PART,  partDSA,  pcaNNet,  pcr,  pda,  pda2,  penalized,  PenalizedLDA,  plr,  pls,  plsRglm,  polr,  ppr,  PRIM,  protoclass,  qda,  QdaCov,  qrf,  qrnn,  randomGLM,  ranger,  rbf,  rbfDDA,  Rborist,  rda,  regLogistic,  relaxo,  rf,  rFerns,  RFlda,  rfRules,  ridge,  rlda,  rlm,  rmda,  rocc,  rotationForest,  rotationForestCp,  rpart,  rpart1SE,  rpart2,  rpartCost,  rpartScore,  rqlasso,  rqnc,  RRF,  RRFglobal,  rrlda,  RSimca,  rvmLinear,  rvmPoly,  rvmRadial,  SBC,  sda,  sdwd,  simpls,  SLAVE,  slda,  smda,  snn,  sparseLDA,  spikeslab,  spls,  stepLDA,  stepQDA,  superpc,  svmBoundrangeString,  svmExpoString,  svmLinear,  svmLinear2,  svmLinear3,  svmLinearWeights,  svmLinearWeights2,  svmPoly,  svmRadial,  svmRadialCost,  svmRadialSigma,  svmRadialWeights,  svmSpectrumString,  tan,  tanSearch,  treebag,  vbmpRadial,  vglmAdjCat,  vglmContRatio,  vglmCumulative,  widekernelpls,  WM,  wsrf,  xgbDART,  xgbLinear,  xgbTree,  xyf'</span>"
      ],
      "text/plain": [
       "[1] \"ada,  AdaBag,  AdaBoost.M1,  adaboost,  amdai,  ANFIS,  avNNet,  awnb,  awtan,  bag,  bagEarth,  bagEarthGCV,  bagFDA,  bagFDAGCV,  bam,  bartMachine,  bayesglm,  binda,  blackboost,  blasso,  blassoAveraged,  bridge,  brnn,  BstLm,  bstSm,  bstTree,  C5.0,  C5.0Cost,  C5.0Rules,  C5.0Tree,  cforest,  chaid,  CSimca,  ctree,  ctree2,  cubist,  dda,  deepboost,  DENFIS,  dnn,  dwdLinear,  dwdPoly,  dwdRadial,  earth,  elm,  enet,  evtree,  extraTrees,  fda,  FH.GBML,  FIR.DM,  foba,  FRBCS.CHI,  FRBCS.W,  FS.HGD,  gam,  gamboost,  gamLoess,  gamSpline,  gaussprLinear,  gaussprPoly,  gaussprRadial,  gbm_h2o,  gbm,  gcvEarth,  GFS.FR.MOGUL,  GFS.LT.RS,  GFS.THRIFT,  glm.nb,  glm,  glmboost,  glmnet_h2o,  glmnet,  glmStepAIC,  gpls,  hda,  hdda,  hdrda,  HYFIS,  icr,  J48,  JRip,  kernelpls,  kknn,  knn,  krlsPoly,  krlsRadial,  lars,  lars2,  lasso,  lda,  lda2,  leapBackward,  leapForward,  leapSeq,  Linda,  lm,  lmStepAIC,  LMT,  loclda,  logicBag,  LogitBoost,  logreg,  lssvmLinear,  lssvmPoly,  lssvmRadial,  lvq,  M5,  M5Rules,  manb,  mda,  Mlda,  mlp,  mlpKerasDecay,  mlpKerasDecayCost,  mlpKerasDropout,  mlpKerasDropoutCost,  mlpML,  mlpSGD,  mlpWeightDecay,  mlpWeightDecayML,  monmlp,  msaenet,  multinom,  mxnet,  mxnetAdam,  naive_bayes,  nb,  nbDiscrete,  nbSearch,  neuralnet,  nnet,  nnls,  nodeHarvest,  null,  OneR,  ordinalNet,  ordinalRF,  ORFlog,  ORFpls,  ORFridge,  ORFsvm,  ownn,  pam,  parRF,  PART,  partDSA,  pcaNNet,  pcr,  pda,  pda2,  penalized,  PenalizedLDA,  plr,  pls,  plsRglm,  polr,  ppr,  PRIM,  protoclass,  qda,  QdaCov,  qrf,  qrnn,  randomGLM,  ranger,  rbf,  rbfDDA,  Rborist,  rda,  regLogistic,  relaxo,  rf,  rFerns,  RFlda,  rfRules,  ridge,  rlda,  rlm,  rmda,  rocc,  rotationForest,  rotationForestCp,  rpart,  rpart1SE,  rpart2,  rpartCost,  rpartScore,  rqlasso,  rqnc,  RRF,  RRFglobal,  rrlda,  RSimca,  rvmLinear,  rvmPoly,  rvmRadial,  SBC,  sda,  sdwd,  simpls,  SLAVE,  slda,  smda,  snn,  sparseLDA,  spikeslab,  spls,  stepLDA,  stepQDA,  superpc,  svmBoundrangeString,  svmExpoString,  svmLinear,  svmLinear2,  svmLinear3,  svmLinearWeights,  svmLinearWeights2,  svmPoly,  svmRadial,  svmRadialCost,  svmRadialSigma,  svmRadialWeights,  svmSpectrumString,  tan,  tanSearch,  treebag,  vbmpRadial,  vglmAdjCat,  vglmContRatio,  vglmCumulative,  widekernelpls,  WM,  wsrf,  xgbDART,  xgbLinear,  xgbTree,  xyf\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#To know what models caret supports, run the following:\n",
    "\n",
    "# See available algorithms in caret\n",
    "modelnames <- paste(names(getModelInfo()), collapse=',  ')\n",
    "modelnames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ec2fa1",
   "metadata": {},
   "source": [
    "## Each of those is a machine learning algorithm caret supports.\n",
    "\n",
    "Yes, it’s a huge list!\n",
    "\n",
    "And if you want to know more details like the hyperparameters and if it can be used of regression or classification problem, then do a modelLookup(algo).\n",
    "\n",
    "Once you have chosen an algorithm, building the model is fairly easy using the train() function.\n",
    "\n",
    "Let’s train a Multivariate Adaptive Regression Splines (MARS) model by setting the method='earth'.\n",
    "\n",
    "The MARS algorithm was named as ‘earth’ in R because of a possible trademark conflict with Salford Systems. May be a rumor. Or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a4e542c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>model</th><th scope=col>parameter</th><th scope=col>label</th><th scope=col>forReg</th><th scope=col>forClass</th><th scope=col>probModel</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>earth         </td><td>nprune        </td><td>#Terms        </td><td>TRUE          </td><td>TRUE          </td><td>TRUE          </td></tr>\n",
       "\t<tr><td>earth         </td><td>degree        </td><td>Product Degree</td><td>TRUE          </td><td>TRUE          </td><td>TRUE          </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " model & parameter & label & forReg & forClass & probModel\\\\\n",
       "\\hline\n",
       "\t earth            & nprune           & \\#Terms         & TRUE             & TRUE             & TRUE            \\\\\n",
       "\t earth          & degree         & Product Degree & TRUE           & TRUE           & TRUE          \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| model | parameter | label | forReg | forClass | probModel |\n",
       "|---|---|---|---|---|---|\n",
       "| earth          | nprune         | #Terms         | TRUE           | TRUE           | TRUE           |\n",
       "| earth          | degree         | Product Degree | TRUE           | TRUE           | TRUE           |\n",
       "\n"
      ],
      "text/plain": [
       "  model parameter label          forReg forClass probModel\n",
       "1 earth nprune    #Terms         TRUE   TRUE     TRUE     \n",
       "2 earth degree    Product Degree TRUE   TRUE     TRUE     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelLookup('earth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c171c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15caa78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using randomForest and predict on the training data itself.\n",
    "\n",
    "model_mars = train(Purchase ~ ., data=trainData, method='earth')\n",
    "fitted <- predict(model_mars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f7ed0d",
   "metadata": {},
   "source": [
    "# But you may ask how is using train() different from using the algorithm’s function directly?\n",
    "\n",
    "The difference is, besides building the model train() does multiple other things like:\n",
    "\n",
    "Cross validating the model\n",
    "\n",
    "Tune the hyper parameters for optimal model performance\n",
    "\n",
    "Choose the optimal model based on a given evaluation metric\n",
    "\n",
    "Preprocess the predictors (what we did so far using preProcess())\n",
    "\n",
    "The train function also accepts the arguments used by the algorithm specified in the method argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eead0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e3cc84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33a2aac6",
   "metadata": {},
   "source": [
    "# Prepare the test dataset and predict\n",
    "\n",
    "A default MARS model has been selected.\n",
    "\n",
    "Now in order to use the model to predict on new data, the data has to be preprocessed and transformed just the way we did on the training data.\n",
    "\n",
    "Thanks to caret, all the information required for pre-processing is stored in the respective preProcess model and dummyVar model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f101c1f4",
   "metadata": {},
   "source": [
    "# You need to pass the testData through these models in the same sequence\n",
    "\n",
    "### preProcess_missingdata_model –> dummies_model –> preProcess_range_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6d7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Impute missing values \n",
    "testData2 <- predict(preProcess_missingdata_model, testData)  \n",
    "\n",
    "# Step 2: Create one-hot encodings (dummy variables)\n",
    "testData3 <- predict(dummies_model, testData2)\n",
    "\n",
    "# Step 3: Transform the features to range between 0 and 1\n",
    "testData4 <- predict(preProcess_range_model, testData3)\n",
    "\n",
    "# View\n",
    "head(testData4[, 1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f080b3a",
   "metadata": {},
   "source": [
    "# Predict on testData\n",
    "The test dataset is prepared. Let’s predict the Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ece58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on testData\n",
    "predicted <- predict(model_mars, testData4)\n",
    "head(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d9855",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "The confusion matrix is a tabular representation to compare the predictions (data) vs the actuals (reference). By setting mode='everything' pretty much most classification evaluation metrics are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "confusionMatrix(reference = testData$Purchase, data = predicted, mode='everything', positive='MM')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251c0c21",
   "metadata": {},
   "source": [
    "# How to do hyperparameter tuning to optimize the model for better performance?\n",
    "\n",
    "There are two main ways to do hyper parameter tuning using the train():\n",
    "\n",
    "Set the tuneLength\n",
    "\n",
    "Define and set the tuneGrid\n",
    "\n",
    "tuneLength corresponds to the number of unique values for the tuning parameters caret will consider while forming the hyper parameter combinations.\n",
    "\n",
    "Caret will automatically determine the values each parameter should take.\n",
    "\n",
    "Alternately, if you want to explicitly control what values should be considered for each parameter, then, you can define the tuneGrid and pass it to train()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d3a9b6",
   "metadata": {},
   "source": [
    "# 7.1. Setting up the trainControl()\n",
    "\n",
    "The train() function takes a trControl argument that accepts the output of trainControl().\n",
    "\n",
    "Inside trainControl() you can control how the train() will:\n",
    "\n",
    "Cross validation method to use.\n",
    "\n",
    "How the results should be summarised using a summary function\n",
    "Cross validation method can be one amongst:\n",
    "\n",
    "‘boot’: Bootstrap sampling\n",
    "\n",
    "‘boot632’: Bootstrap sampling with 63.2% bias correction applied\n",
    "\n",
    "‘optimism_boot’: The optimism bootstrap estimator\n",
    "\n",
    "‘boot_all’: All boot methods.\n",
    "\n",
    "‘cv’: k-Fold cross validation\n",
    "\n",
    "‘repeatedcv’: Repeated k-Fold cross validation\n",
    "\n",
    "‘oob’: Out of Bag cross validation\n",
    "\n",
    "‘LOOCV’: Leave one out cross validation\n",
    "\n",
    "‘LGOCV’: Leave group out cross validation\n",
    "\n",
    "The summaryFunction can be twoClassSummary if Y is binary class or multiClassSummary if the Y has more than 2 categories.\n",
    "\n",
    "By settiung the classProbs=T the probability scores are generated instead of directly predicting the class based on a predetermined cutoff of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb1ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training control\n",
    "fitControl <- trainControl(\n",
    "    method = 'cv',                   # k-fold cross validation\n",
    "    number = 5,                      # number of folds\n",
    "    savePredictions = 'final',       # saves predictions for optimal tuning parameter\n",
    "    classProbs = T,                  # should class probabilities be returned\n",
    "    summaryFunction=twoClassSummary  # results summary function\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20bcf8",
   "metadata": {},
   "source": [
    "# 7.2 Hyper Parameter Tuning using tuneLength\n",
    "\n",
    "Let’s take the train() function we used before, plus, additionally set the tuneLength, trControl and metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622bd45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Tune hyper parameters by setting tuneLength\n",
    "set.seed(100)\n",
    "model_mars2 = train(Purchase ~ ., data=trainData, method='earth', tuneLength = 5, metric='ROC', trControl = fitControl)\n",
    "model_mars2\n",
    "\n",
    "# Step 2: Predict on testData and Compute the confusion matrix\n",
    "predicted2 <- predict(model_mars2, testData4)\n",
    "confusionMatrix(reference = testData$Purchase, data = predicted2, mode='everything', positive='MM')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93733c99",
   "metadata": {},
   "source": [
    "# 7.3. Hyper Parameter Tuning using tuneGrid\n",
    "\n",
    "Alternately, you can set the tuneGrid instead of tuneLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0432342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the tuneGrid\n",
    "marsGrid <-  expand.grid(nprune = c(2, 4, 6, 8, 10), \n",
    "                        degree = c(1, 2, 3))\n",
    "\n",
    "# Step 2: Tune hyper parameters by setting tuneGrid\n",
    "set.seed(100)\n",
    "model_mars3 = train(Purchase ~ ., data=trainData, method='earth', metric='ROC', tuneGrid = marsGrid, trControl = fitControl)\n",
    "model_mars3\n",
    "\n",
    "# Step 3: Predict on testData and Compute the confusion matrix\n",
    "predicted3 <- predict(model_mars3, testData4)\n",
    "confusionMatrix(reference = testData$Purchase, data = predicted3, mode='everything', positive='MM')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d900410b",
   "metadata": {},
   "source": [
    "# 8. How to evaluate performance of multiple machine learning algorithms?\n",
    "\n",
    "Caret provides the resamples() function where you can provide multiple machine learning models and collectively evaluate them.\n",
    "\n",
    "Let’s first train some more algorithms.\n",
    "\n",
    "# 8.1. Training Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab86808",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(100)\n",
    "\n",
    "# Train the model using adaboost\n",
    "model_adaboost = train(Purchase ~ ., data=trainData, method='adaboost', tuneLength=2, trControl = fitControl)\n",
    "model_adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d10d42",
   "metadata": {},
   "source": [
    "# 8.2. Training Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81592cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(100)\n",
    "\n",
    "# Train the model using rf\n",
    "model_rf = train(Purchase ~ ., data=trainData, method='rf', tuneLength=5, trControl = fitControl)\n",
    "model_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087d08e3",
   "metadata": {},
   "source": [
    "# 8.3. Training xgBoost Dart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f1970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(100)\n",
    "\n",
    "# Train the model using MARS\n",
    "model_xgbDART = train(Purchase ~ ., data=trainData, method='xgbDART', tuneLength=5, trControl = fitControl, verbose=F)\n",
    "model_xgbDART"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae4150",
   "metadata": {},
   "source": [
    "# 8.4. Training SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81330517",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(100)\n",
    "\n",
    "# Train the model using MARS\n",
    "model_svmRadial = train(Purchase ~ ., data=trainData, method='svmRadial', tuneLength=15, trControl = fitControl)\n",
    "model_svmRadial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89625f1",
   "metadata": {},
   "source": [
    "# 8.5. Run resamples() to compare the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602fca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performances using resample()\n",
    "models_compare <- resamples(list(ADABOOST=model_adaboost, RF=model_rf, XGBDART=model_xgbDART, MARS=model_mars3, SVM=model_svmRadial))\n",
    "\n",
    "# Summary of the models performances\n",
    "summary(models_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2485636d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5bdb20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
